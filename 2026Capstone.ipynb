{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZoy4nEtaYGsT4jDBuK6uY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arminmu13106-art/TEAM7-Capstone-Project/blob/main/2026Capstone.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vlbRDi56WCUa"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Predicting Career Domain and Seniority from LinkedIn Profiles**\n",
        "\n",
        "Project Overview:\n",
        "\n",
        "In this semester’s capstone project, your task is to develop an end-to-end machine-learning pipeline that predicts\n",
        "\n",
        "(1) the current professional domain and\n",
        "\n",
        "(2) the current seniority level of an individual based solely on the information contained in their LinkedIn CV.\n",
        "\n",
        "Your models will be evaluated using a hand-labeled dataset provided by SnapAddy.\n",
        "The project encourages you to creatively combine modern NLP techniques, programmatic labeling strategies, and supervised or zero-shot approaches to extract meaningful signals from semi-structured career data.\n",
        "\n",
        "Details:\n",
        "The target is to predict the characteristics (domain, seniority) of the current job. The current job is labeled as \"ACTIVE\" in the status in the CVs.\n",
        "\n",
        "Possible Approaches (non-exhaustive)\n",
        "\n",
        "1.Rule-based matching (baseline): Identify relevant job titles and text passages using predefined label lists and assign domain and seniority accordingly.\n",
        "\n",
        "2.Embedding-based labeling: Use the provided label lists to generate embeddings (e.g., via LLMs or sentence transformers). Compute similarity between profile text and label embeddings and perform zero-shot classification.\n",
        "\n",
        "*3.Fine-tuned classification model. Use the csv files to fine-tune a pre-trained classification model. Apply the model to the linked-in data*\n",
        "\n",
        "4.Programmatic labeling + supervised learning: Use rule-based or embedding-based predictions to create pseudo-labels for a large set of LinkedIn profiles, then fine-tune a classifier on this expanded dataset.\n",
        "\n",
        "5.Feature engineering and conventional machine learning. Look at the linked-In data and generate meaningful features (e.g. number of previous jobs as an indicator for seniority, etc.) . Then train conventional algorithms (e.g. random forests) to predict the labels.\n",
        "\n",
        "6.Simple interpretable baseline: E.g. a bag-of-words and TF–IDF + logistic regression classifier for domain or seniority.\n",
        "\n",
        "7.Your own approach: Be creative and find your own solution.\n",
        "\n",
        "Note that for each of these approaches, two models are required: one for predicting the department and one for predicting the seniority."
      ],
      "metadata": {
        "id": "FDA4INuGWTX2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download（json&csv）"
      ],
      "metadata": {
        "id": "Pu4x1ctdeKm2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "DJVK133G9HXr",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "dYXZ9DQr1UPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(\"test_json.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data_json = json.load(f)\n",
        "\n",
        "print(type(data_json))\n",
        "print(len(data_json))\n",
        "print(data_json[0])"
      ],
      "metadata": {
        "id": "uU8fMU52jLQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's json file. Then CSV file."
      ],
      "metadata": {
        "id": "ib_5ZDgaeSke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()\n",
        "# department-v2"
      ],
      "metadata": {
        "id": "A1mqcT_tWSXm",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()\n",
        "# seniority-v2"
      ],
      "metadata": {
        "id": "GW-H7q3S9uJ_",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "nFGrKmx2BN3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_department = pd.read_csv(\"department-v2.csv\")\n",
        "df_seniority = pd.read_csv(\"seniority-v2.csv\")\n",
        "\n",
        "df_department.head(), df_seniority.head()"
      ],
      "metadata": {
        "id": "IrJ04Q-B_g8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First do the model for DEPARTMENT"
      ],
      "metadata": {
        "id": "_2oiCKGc7NcS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_department['text'].str.len().describe()"
      ],
      "metadata": {
        "id": "Zd88q6fpByK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_department['label'].value_counts(normalize=True)"
      ],
      "metadata": {
        "id": "Fw1WhIxS7Qkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Method：**TF-IDF + Logistic Regression**\n",
        "\n",
        "Term Frequency – Inverse Document Frequency"
      ],
      "metadata": {
        "id": "BXUzfczKFj63"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No suitable for deep learning\n",
        "because:\n",
        "\n",
        "1. The text is extremely short (average 34 characters)\n",
        "\n",
        "2. The dataset size is approximately 10k (too small for DL)\n",
        "\n",
        "3. Category noise is high (the title itself is ambiguous)\n",
        "\n",
        "4. The task objective leans toward semantic matching rather than generation"
      ],
      "metadata": {
        "id": "2kvYVH92GO-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df_department[\"text\"]\n",
        "y = df_department[\"label\"]\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X,\n",
        "    y,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "print(len(X_train), len(X_val))"
      ],
      "metadata": {
        "id": "xv-qWlDj7Qfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer(\n",
        "    lowercase=True,\n",
        "    ngram_range=(1, 2),\n",
        "    min_df=2,\n",
        "    max_df=0.9\n",
        ")"
      ],
      "metadata": {
        "id": "sNbuqKU67QZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_tfidf = tfidf.fit_transform(X_train)"
      ],
      "metadata": {
        "id": "3F7JmI7v7y87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_val_tfidf = tfidf.transform(X_val)"
      ],
      "metadata": {
        "id": "vQxewjjT7y0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train_tfidf.shape)\n",
        "print(X_val_tfidf.shape)"
      ],
      "metadata": {
        "id": "ILAAEjWqDqmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modeling"
      ],
      "metadata": {
        "id": "45ja1J2XFWJ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "clf = LogisticRegression(\n",
        "    max_iter=1000,\n",
        "    class_weight=\"balanced\",\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "pCNAFXIcDqhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf.fit(X_train_tfidf, y_train)"
      ],
      "metadata": {
        "id": "6vRWX9eaDqbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = clf.predict(X_val_tfidf)"
      ],
      "metadata": {
        "id": "syWDHPGvJEaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_val, y_pred))"
      ],
      "metadata": {
        "id": "6JjSJJr_JEVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import re\n",
        "\n",
        "def text_augment_case_symbol(text):\n",
        "    ops = []\n",
        "\n",
        "    ops.append(text.lower())\n",
        "    ops.append(text.upper())\n",
        "    ops.append(text.title())\n",
        "\n",
        "    ops.append(text.replace(\" \", \"-\"))\n",
        "    ops.append(text.replace(\" \", \"_\"))\n",
        "\n",
        "    ops.append(re.sub(r\"\\s+\", \"  \", text))\n",
        "\n",
        "    ops = list(set(ops))\n",
        "    ops = [t for t in ops if t != text]\n",
        "\n",
        "    if not ops:\n",
        "        return text\n",
        "\n",
        "    return random.choice(ops)\n"
      ],
      "metadata": {
        "id": "E4MGIAIkJES3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = df_department.copy()\n",
        "\n",
        "class_counts = df['label'].value_counts()\n",
        "\n",
        "small_classes = class_counts[class_counts < 50].index.tolist()\n",
        "\n",
        "augmented_rows = []\n",
        "\n",
        "for cls in small_classes:\n",
        "    subset = df[df['label'] == cls]\n",
        "\n",
        "    for _, row in subset.iterrows():\n",
        "        if random.random() < 0.8:\n",
        "            new_text = text_augment_case_symbol(row['text'])\n",
        "            augmented_rows.append({\n",
        "                \"text\": new_text,\n",
        "                \"label\": cls\n",
        "            })\n",
        "\n",
        "df_augmented = pd.concat([df, pd.DataFrame(augmented_rows)], ignore_index=True)\n",
        "\n",
        "print(\"Original:\", len(df))\n",
        "print(\"After:\", len(df_augmented))\n"
      ],
      "metadata": {
        "id": "RFMQVfSU4vl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "“We applied light text augmentation limited to casing and formatting variations to improve robustness against real-world writing inconsistencies, particularly for underrepresented classes, without introducing semantic noise.”"
      ],
      "metadata": {
        "id": "NaxTBmTu6hmf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "X = df_augmented['text']\n",
        "y = df_augmented['label']\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_val_tfidf = vectorizer.transform(X_val)\n",
        "\n",
        "clf = LogisticRegression(\n",
        "    max_iter=1000,\n",
        "    class_weight=\"balanced\",\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "clf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_val_tfidf)\n",
        "\n",
        "print(classification_report(y_val, y_pred))\n"
      ],
      "metadata": {
        "id": "ul7I0WcX6eB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above represents the optimized results of data augmentation (case, spacing, and symbol variations)."
      ],
      "metadata": {
        "id": "I4qP2-4mCUvj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, try n-gram TF-IDF. The default raw version is Unigram, which handles individual words. N-grams represent phrases."
      ],
      "metadata": {
        "id": "vxLo2zz7Cdij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_department"
      ],
      "metadata": {
        "id": "QmNcWSmCCc_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df_department['text']\n",
        "y = df_department['label']\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X,\n",
        "    y,\n",
        "    test_size=0.2,\n",
        "    stratify=y,\n",
        "    random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "qpwFlVYECiTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(\n",
        "    ngram_range=(1, 2),   # unigram + bigram\n",
        "    min_df=2,\n",
        "    max_df=0.95,          # remove most frequency word\n",
        "    lowercase=True\n",
        ")\n",
        "\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_val_tfidf = vectorizer.transform(X_val)\n",
        "\n",
        "print(X_train_tfidf.shape)\n",
        "print(X_val_tfidf.shape)"
      ],
      "metadata": {
        "id": "Z4nLXm2qCiCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "clf = LogisticRegression(\n",
        "    max_iter=1000,\n",
        "    class_weight=\"balanced\",\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "clf.fit(X_train_tfidf, y_train)"
      ],
      "metadata": {
        "id": "afnZqUfFHkYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "y_pred = clf.predict(X_val_tfidf)\n",
        "\n",
        "print(classification_report(y_val, y_pred))\n"
      ],
      "metadata": {
        "id": "pPk9xR_JmnXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next combine **enhanced data+ n-gram**"
      ],
      "metadata": {
        "id": "071T-pOXoVCC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_aug = df_augmented['text']\n",
        "y_aug = df_augmented['label']\n",
        "\n",
        "print(\"Original size:\", len(df_department))\n",
        "print(\"Augmented size:\", len(df_augmented))\n",
        "\n",
        "y_aug.value_counts(normalize=True)"
      ],
      "metadata": {
        "id": "UNppf__MmnHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train_aug, X_val_aug, y_train_aug, y_val_aug = train_test_split(\n",
        "    X_aug,\n",
        "    y_aug,\n",
        "    test_size=0.2,\n",
        "    stratify=y_aug,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(len(X_train_aug), len(X_val_aug))"
      ],
      "metadata": {
        "id": "Jo0f_cvNolc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer_aug_ngram = TfidfVectorizer(\n",
        "    ngram_range=(1, 2),\n",
        "    min_df=2\n",
        ")\n",
        "\n",
        "X_train_aug_tfidf = vectorizer_aug_ngram.fit_transform(X_train_aug)\n",
        "X_val_aug_tfidf = vectorizer_aug_ngram.transform(X_val_aug)\n",
        "\n",
        "print(X_train_aug_tfidf.shape)\n",
        "print(X_val_aug_tfidf.shape)"
      ],
      "metadata": {
        "id": "ila8bkUzolXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "clf_aug_ngram = LogisticRegression(\n",
        "    max_iter=1000,\n",
        "    class_weight=\"balanced\",\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Training with TF-IDF features enhanced by n-grams\n",
        "clf_aug_ngram.fit(X_train_aug_tfidf, y_train_aug)\n",
        "\n",
        "y_pred_aug_ngram = clf_aug_ngram.predict(X_val_aug_tfidf)\n",
        "\n",
        "print(classification_report(y_val_aug, y_pred_aug_ngram))"
      ],
      "metadata": {
        "id": "OIX9bmRRolJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try L1 regularization."
      ],
      "metadata": {
        "id": "PCLlHcWkq_Pn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# use X_train_tfidf, X_val_tfidf, y_train, y_val\n",
        "clf_l1 = LogisticRegression(\n",
        "    penalty='l1',\n",
        "    solver='liblinear',\n",
        "    C=1.0,          ######1\n",
        "    class_weight='balanced',\n",
        "    max_iter=1000,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "clf_l1.fit(X_train_tfidf, y_train)\n",
        "\n",
        "y_pred_l1 = clf_l1.predict(X_val_tfidf)\n",
        "\n",
        "print(classification_report(y_val, y_pred_l1))"
      ],
      "metadata": {
        "id": "6nOB2aeqrB4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "clf_l1 = LogisticRegression(\n",
        "    penalty='l1',\n",
        "    solver='liblinear',\n",
        "    C=0.5,               #0.5\n",
        "    class_weight='balanced',\n",
        "    max_iter=1000,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "clf_l1.fit(X_train_tfidf, y_train)\n",
        "\n",
        "y_pred_l1 = clf_l1.predict(X_val_tfidf)\n",
        "\n",
        "print(classification_report(y_val, y_pred_l1))"
      ],
      "metadata": {
        "id": "ikVR199NJh43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "clf_l1 = LogisticRegression(\n",
        "    penalty='l1',\n",
        "    solver='liblinear',\n",
        "    C=2,               #2\n",
        "    class_weight='balanced',\n",
        "    max_iter=1000,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "clf_l1.fit(X_train_tfidf, y_train)\n",
        "\n",
        "y_pred_l1 = clf_l1.predict(X_val_tfidf)\n",
        "\n",
        "print(classification_report(y_val, y_pred_l1))"
      ],
      "metadata": {
        "id": "yttr1Y81Jhzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "C=1 is the best."
      ],
      "metadata": {
        "id": "hKL1M-Ftz004"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df_augmented[\"text\"]\n",
        "y = df_augmented[\"label\"]\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X,\n",
        "    y,\n",
        "    test_size=0.2,\n",
        "    stratify=y,\n",
        "    random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "qWEFPGHdxIix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(\n",
        "    ngram_range=(1, 2),     # unigram + bigram\n",
        "    min_df=2,\n",
        "    max_df=0.9\n",
        ")\n",
        "\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_val_tfidf = vectorizer.transform(X_val)\n",
        "\n",
        "print(\"Train TF-IDF shape:\", X_train_tfidf.shape)\n",
        "print(\"Val   TF-IDF shape:\", X_val_tfidf.shape)"
      ],
      "metadata": {
        "id": "xu3BJm7s1X7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = LogisticRegression(\n",
        "    penalty=\"l1\",\n",
        "    solver=\"liblinear\",\n",
        "    C=1.0,\n",
        "    class_weight=\"balanced\",\n",
        "    max_iter=1000,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "clf.fit(X_train_tfidf, y_train)"
      ],
      "metadata": {
        "id": "-J9Q5l1k1XtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = clf.predict(X_val_tfidf)\n",
        "\n",
        "print(\"===== Final Department Model (Validation) =====\")\n",
        "print(classification_report(y_val, y_pred))"
      ],
      "metadata": {
        "id": "ONq3_Hwj1aBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final enhanced TF-IDF + Logistic Regression (L1) model（department）"
      ],
      "metadata": {
        "id": "cB7nRfrIkEim"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LETS GOOO , TEST json"
      ],
      "metadata": {
        "id": "Z6JypBHal5Y6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "type(data_json)"
      ],
      "metadata": {
        "id": "iauaIb0YaK9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(data_json[0][0])"
      ],
      "metadata": {
        "id": "Xr7nQ-R1mk68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_json[0][0].keys()"
      ],
      "metadata": {
        "id": "h2qtaH2Wm22r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts_test = []\n",
        "\n",
        "for group in data_json:\n",
        "    for item in group:\n",
        "        texts_test.append(item[\"position\"])\n",
        "\n",
        "print(\"Number of test samples:\", len(texts_test))"
      ],
      "metadata": {
        "id": "BP8KezMAm20h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = vectorizer.transform(texts_test)\n",
        "print(\"TF-IDF test shape:\", X_test.shape)"
      ],
      "metadata": {
        "id": "1EaDfaipm2oT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = [record[0]['position'] for record in data_json]\n",
        "y_true_department = [record[0]['department'] for record in data_json]\n",
        "\n",
        "print(\"Number of samples:\", len(X_test), len(y_true_department))\n",
        "\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "y_pred_department = clf.predict(X_test_tfidf)\n",
        "\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_true_department, y_pred_department))\n",
        "print(\"Macro F1:\", f1_score(y_true_department, y_pred_department, average='macro'))\n",
        "print(\"Weighted F1:\", f1_score(y_true_department, y_pred_department, average='weighted'))\n",
        "\n",
        "print(\"\\nDetailed report:\")\n",
        "print(classification_report(y_true_department, y_pred_department))"
      ],
      "metadata": {
        "id": "SA8OXhyPvwja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seniority！！"
      ],
      "metadata": {
        "id": "IJ9YhJdf3A4x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_seniority['label'].value_counts(normalize=True)"
      ],
      "metadata": {
        "id": "bVJeQc8Bvwbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_seniority['text'].str.len().describe()"
      ],
      "metadata": {
        "id": "osJ4131U6p7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_seniority.head(5)"
      ],
      "metadata": {
        "id": "HlAHeRvX6pz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df_seniority['text']\n",
        "y = df_seniority['label']\n",
        "\n",
        "df_seniority_train, df_seniority_test = train_test_split(\n",
        "    df_seniority,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "print(\"Train label distribution:\")\n",
        "print(df_seniority_train['label'].value_counts(normalize=True))\n",
        "\n",
        "print(\"\\nTest label distribution:\")\n",
        "print(df_seniority_test['label'].value_counts(normalize=True))\n"
      ],
      "metadata": {
        "id": "-BfHHmXcPl-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
        "\n",
        "# X / y\n",
        "X_train = df_seniority_train['text']\n",
        "y_train = df_seniority_train['label']\n",
        "\n",
        "X_test = df_seniority_test['text']\n",
        "y_test = df_seniority_test['label']\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(\n",
        "        ngram_range=(1, 2),\n",
        "        min_df=5,\n",
        "        max_df=0.9\n",
        "    )),\n",
        "    ('clf', LogisticRegression(\n",
        "        penalty='l1',\n",
        "        solver='liblinear',\n",
        "        class_weight='balanced',\n",
        "        max_iter=1000\n",
        "    ))\n",
        "])\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Macro F1:\", f1_score(y_test, y_pred, average='macro'))\n",
        "print(\"Weighted F1:\", f1_score(y_test, y_pred, average='weighted'))\n",
        "\n",
        "print(\"\\nDetailed report:\\n\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "SL_qKwfnJQbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try L1 regularization."
      ],
      "metadata": {
        "id": "nIH8tBmbbY7P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df_seniority['text']\n",
        "y = df_seniority['label']\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_val_tfidf = vectorizer.transform(X_val)\n",
        "\n",
        "clf_seniority_l1 = LogisticRegression(\n",
        "    penalty='l1',\n",
        "    solver='liblinear',\n",
        "    C=1.0,\n",
        "    class_weight='balanced',\n",
        "    max_iter=1000,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# train\n",
        "clf_seniority_l1.fit(X_train_tfidf, y_train)\n",
        "\n",
        "y_pred_l1 = clf_seniority_l1.predict(X_val_tfidf)\n",
        "\n",
        "print(\"===== L1 Regularized Seniority Model Performance =====\")\n",
        "print(f\"Number of samples: {X_val_tfidf.shape[0]} {len(y_pred_l1)}\")\n",
        "print(classification_report(y_val, y_pred_l1))\n"
      ],
      "metadata": {
        "id": "pkYIHinjJQE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "test on json, with L1 regularization."
      ],
      "metadata": {
        "id": "PueZhu0TbsPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Seniority Model Evaluation on JSON (with L1)\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, f1_score, accuracy_score\n",
        "\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "for sublist in data_json:\n",
        "    for entry in sublist:\n",
        "        position_text = entry['position']\n",
        "        true_label = entry['seniority']\n",
        "        X_vec = vectorizer.transform([position_text])\n",
        "        pred_label = clf_seniority_l1.predict(X_vec)[0]\n",
        "\n",
        "        y_true.append(true_label)\n",
        "        y_pred.append(pred_label)\n",
        "\n",
        "y_true = np.array(y_true)\n",
        "y_pred = np.array(y_pred)\n",
        "\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
        "weighted_f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "print(f\"Number of samples: {len(y_true)} {len(y_pred)}\")\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Macro F1: {macro_f1}\")\n",
        "print(f\"Weighted F1: {weighted_f1}\")\n",
        "print(\"\\nDetailed report:\")\n",
        "print(classification_report(y_true, y_pred))"
      ],
      "metadata": {
        "id": "PjjttqlcaEdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original version without L1"
      ],
      "metadata": {
        "id": "JdZK_Pklh6LT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_json = [item[0]['position'] for item in data_json]\n",
        "\n",
        "y_pred_json = pipeline.predict(X_test_json)\n",
        "y_true_json = [item[0]['seniority'] for item in data_json]\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "\n",
        "print(\"Number of samples:\", len(y_true_json), len(y_pred_json))\n",
        "print(\"Accuracy:\", accuracy_score(y_true_json, y_pred_json))\n",
        "print(\"Macro F1:\", f1_score(y_true_json, y_pred_json, average='macro'))\n",
        "print(\"Weighted F1:\", f1_score(y_true_json, y_pred_json, average='weighted'))\n",
        "print(\"\\nDetailed report:\\n\")\n",
        "print(classification_report(y_true_json, y_pred_json))"
      ],
      "metadata": {
        "id": "ABq7qcCSamdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iw3PZ1y2I-7y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}